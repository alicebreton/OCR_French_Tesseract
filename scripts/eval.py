import os
import numpy as np

def wer(reference, hypothesis):
    """
    Calculate the Word Error Rate (WER) between a reference text and a hypothesis text.
    Also, return the list of incorrect tokens (substitutions, deletions, and insertions).
    
    Args:
    reference (str): The ground truth text.
    hypothesis (str): The text generated by the system.
    
    Returns:
    float: The WER as a percentage.
    list: A list of incorrect tokens.
    """
    
    # Remove newlines and split the reference and hypothesis texts into words
    ref_words = reference.replace('\n', ' ').split()
    hyp_words = hypothesis.replace('\n', ' ').split()

    # Create a matrix to store the costs of transformations, using np.uint16 to avoid overflow
    d = np.zeros((len(ref_words) + 1, len(hyp_words) + 1), dtype=np.uint16)

    # Initialize the matrix
    for i in range(len(ref_words) + 1):
        d[i][0] = i
    for j in range(len(hyp_words) + 1):
        d[0][j] = j

    # Compute the cost of substitutions, deletions, and insertions
    for i in range(1, len(ref_words) + 1):
        for j in range(1, len(hyp_words) + 1):
            if ref_words[i - 1] == hyp_words[j - 1]:
                d[i][j] = d[i - 1][j - 1]
            else:
                substitution = d[i - 1][j - 1] + 1
                insertion = d[i][j - 1] + 1
                deletion = d[i - 1][j] + 1
                d[i][j] = min(substitution, insertion, deletion)

    # Backtrack to find the incorrect tokens
    i, j = len(ref_words), len(hyp_words)
    incorrect_tokens = []
    
    while i > 0 or j > 0:
        if i > 0 and j > 0 and ref_words[i - 1] == hyp_words[j - 1]:
            i -= 1
            j -= 1
        elif i > 0 and j > 0 and d[i][j] == d[i - 1][j - 1] + 1:
            incorrect_tokens.append(f"Substitution: '{ref_words[i - 1]}' -> '{hyp_words[j - 1]}'")
            i -= 1
            j -= 1
        elif j > 0 and d[i][j] == d[i][j - 1] + 1:
            incorrect_tokens.append(f"Insertion: '{hyp_words[j - 1]}'")
            j -= 1
        elif i > 0 and d[i][j] == d[i - 1][j] + 1:
            incorrect_tokens.append(f"Deletion: '{ref_words[i - 1]}'")
            i -= 1

    # The WER is the number of edits divided by the number of words in the reference
    wer_value = d[len(ref_words)][len(hyp_words)] / len(ref_words)
    
    return wer_value, incorrect_tokens


def read_file(file_path):
    """
    Read the contents of a text file.
    
    Args:
    file_path (str): Path to the text file.
    
    Returns:
    str: The content of the file as a single string.
    """
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read().strip()
    return content

def calculate_wer_for_files(ref_folder, hyp_folder, output_file):
    """
    Calculate the WER for multiple pairs of reference and hypothesis text files.
    Also, output the incorrect tokens for each pair and calculate the mean WER.
    
    Args:
    ref_folder (str): Path to the folder containing reference text files.
    hyp_folder (str): Path to the folder containing hypothesis text files.
    output_file (str): Path to the output text file.
    """
    
    ref_files = [f for f in os.listdir(ref_folder) if f.endswith('_ref.txt')]

    total_wer = 0
    file_count = 0

    with open(output_file, 'w', encoding='utf-8') as out_file:
        for ref_file in ref_files:
            hyp_file = ref_file.replace('_ref', '')
            
            ref_file_path = os.path.join(ref_folder, ref_file)
            hyp_file_path = os.path.join(hyp_folder, hyp_file)

            if not os.path.exists(hyp_file_path):
                out_file.write(f"Hypothesis file not found for: {ref_file}\n")
                continue
            
            reference_text = read_file(ref_file_path)
            hypothesis_text = read_file(hyp_file_path)

            wer_score, incorrect_tokens = wer(reference_text, hypothesis_text)
            
            total_wer += wer_score
            file_count += 1
            
            out_file.write(f"WER for {ref_file} and {hyp_file}: {wer_score:.2%}\n")
            if incorrect_tokens:
                out_file.write("Incorrect tokens:\n")
                for token in incorrect_tokens:
                    out_file.write(f" - {token}\n")
            else:
                out_file.write("No errors found.\n")
            out_file.write("\n")
        
        if file_count > 0:
            mean_wer = total_wer / file_count
            out_file.write(f"Mean WER for all files: {mean_wer:.2%}\n")
        else:
            out_file.write("No files processed.\n")

reference_folder_path = './data/ref/'
hypothesis_folder_path = './data/output/'
output_file_path = './wer_results.txt'

calculate_wer_for_files(reference_folder_path, hypothesis_folder_path, output_file_path)
